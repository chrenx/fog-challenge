{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 \n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv1D, MaxPooling1D, UpSampling1D, Conv1DTranspose,Lambda,add, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(11)\n",
    "\n",
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "\n",
    "def build_autoencoder(size,channel): #2048\n",
    "\n",
    "    inputs = Input(shape=[size,channel])\n",
    "\n",
    "    ## encoding layers\n",
    "    conv1 = Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv1D(32, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(conv1) #1024\n",
    "\n",
    "    conv2 = Conv1D(64, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv1D(64, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(conv2) #512\n",
    "\n",
    "    conv3 = Conv1D(128, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv1D(128, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(conv3) #256\n",
    "\n",
    "    conv4 = Conv1D(256, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv1D(256, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=2)(conv4) #128\n",
    "\n",
    "    conv4_2 = Conv1D(512, 3, activation='relu', padding='same')(pool4)\n",
    "    conv4_2 = Conv1D(512, 3, activation='relu', padding='same')(conv4_2) #128*512\n",
    "    pool4_2 = MaxPooling1D(pool_size=2)(conv4_2)  #64*512\n",
    "\n",
    "\n",
    "    #decoding layers\n",
    "    conv5_2 = Conv1D(512, 3, activation='relu', padding='same')(pool4_2) #64*512\n",
    "    conv5_2 = Conv1D(512, 3, activation='relu', padding='same')(conv5_2)\n",
    "    up1_2 = concatenate([Conv1DTranspose(512, 2, strides=2, activation='relu',padding='same')(conv5_2),conv4_2],axis=-1) # conv5_2 conv1dtranspose: 128*512, after concat: 128*1024(up1_2)\n",
    "    #up1_2 = Conv1DTranspose(512, 2, strides=2, activation='relu',padding='same')(conv5_2) # if you want to do full, then no concate\n",
    "\n",
    "    conv5 = Conv1D(256, 3, activation='relu', padding='same')(up1_2) #128*256\n",
    "    conv5 = Conv1D(256, 3, activation='relu', padding='same')(conv5)\n",
    "    up1 = concatenate([Conv1DTranspose(512, 2, strides=2, activation='relu',padding='same')(conv5),conv4],axis=-1) #256*XXX\n",
    "\n",
    "    conv6 = Conv1D(128, 3, activation='relu', padding='same')(up1)\n",
    "    conv6 = Conv1D(128, 3, activation='relu', padding='same')(conv6)\n",
    "    up2 = concatenate([Conv1DTranspose(512, 2, strides=2, activation='relu',padding='same')(conv6),conv3],axis=-1) \n",
    "\n",
    "    conv7= Conv1D(64, 3, activation='relu', padding='same')(up2)\n",
    "    conv7= Conv1D(64, 3, activation='relu', padding='same')(conv7)\n",
    "    up3 = concatenate([Conv1DTranspose(512, 2, strides=2, activation='relu',padding='same')(conv7),conv2],axis=-1)\n",
    "\n",
    "    conv8= Conv1D(64, 3, activation='relu', padding='same')(up3)\n",
    "    conv8= Conv1D(64, 3, activation='relu', padding='same')(conv8)\n",
    "    up4 = concatenate([Conv1DTranspose(512, 2, strides=2, activation='relu',padding='same')(conv8),conv1],axis=-1)\n",
    "    decoding = Conv1D(1, 3, padding ='same')(up4)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[decoding])\n",
    "    model.compile(optimizer=Adam(learning_rate=3e-5, beta_1=0.9, beta_2=0.999, decay=0.0), loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2048, 3), array([[0.18026969, 0.01947524, 0.46321853]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "image_batch = np.random.rand(1,2048,3)\n",
    "image_batch2 = image_batch.copy()\n",
    "image_batch.shape, image_batch[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1, 2048, 1),\n",
       " array([[[-0.03198639],\n",
       "         [-0.03062092],\n",
       "         [-0.05372105],\n",
       "         ...,\n",
       "         [-0.03960337],\n",
       "         [-0.00461557],\n",
       "         [-0.00429596]]], dtype=float32))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_autoencoder(2048,3)\n",
    "\n",
    "output = model.predict(image_batch)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Define the autoencoder model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, size, channel):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(channel, 32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv1d(1024, 256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv1d(512, 128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(512, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv1d(256, 64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(512, 64, kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv1d(128, 1, kernel_size=3, padding='same')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        dec = self.decoder(enc)\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2048  # Example sequence length\n",
    "channel = 3  # Example number of channels\n",
    "\n",
    "model2 = UNet(size, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv1d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, str, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int,)!, !str!, !tuple of (int,)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int,)!, str, !tuple of (int,)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model2\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     output2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m output2\u001b[38;5;241m.\u001b[39mshape, output2\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 72\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 72\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(enc)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/disk4/chrenx/conda-env/envs/fog/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv1d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, str, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int,)!, !str!, !tuple of (int,)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int,)!, str, !tuple of (int,)!, int)\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    output2 = model2(image_batch2)\n",
    "output2.shape, output2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count parameterts in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of parameters\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"A dictionary with dot notation access to attributes.\"\"\"\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "    def __delattr__(self, key):\n",
    "        try:\n",
    "            del self[key]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n",
    "        \n",
    "# Function to convert existing dictionary to DotDict\n",
    "def dict_to_dotdict(d):\n",
    "    if not isinstance(d, dict):\n",
    "        return d\n",
    "    return DotDict({k: dict_to_dotdict(v) if isinstance(v, dict) else v for k, v in d.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBiLSTM Total parameters: 7486402\n",
      "TransformerBiLSTM Trainable parameters: 7486402\n",
      "Transformer Total parameters: 3379522\n",
      "Transformer Trainable parameters: 3379522\n"
     ]
    }
   ],
   "source": [
    "from models.transformer_bilstm_v2 import TransformerBiLSTM\n",
    "from models.transformer_v1 import Transformer\n",
    "opt = {\n",
    "    'block_size': 15552,\n",
    "    'block_stride': 972,\n",
    "    'patch_size': 18,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'fog_model_input_dim': 54,\n",
    "    'fog_model_dim': 320,\n",
    "    'fog_model_num_heads': 8,\n",
    "    'fog_model_num_encoder_layers': 5,\n",
    "    'fog_model_num_lstm_layers': 2,\n",
    "    'fog_model_first_dropout': 0.1,\n",
    "    'fog_model_encoder_dropout': 0.1,\n",
    "    'fog_model_mha_dropout': 0.0,\n",
    "}\n",
    "\n",
    "opt = DotDict(opt)\n",
    "\n",
    "model = TransformerBiLSTM(opt)\n",
    "transformer_model = Transformer(opt)\n",
    "\n",
    "# Get the total and trainable parameters\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "print(f\"TransformerBiLSTM Total parameters: {total_params}\")\n",
    "print(f\"TransformerBiLSTM Trainable parameters: {trainable_params}\")\n",
    "\n",
    "total_params, trainable_params = count_parameters(transformer_model)\n",
    "\n",
    "print(f\"Transformer Total parameters: {total_params}\")\n",
    "print(f\"Transformer Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n",
      "dict_keys(['series_name', 'start_t_idx', 'end_t_idx', 'model_input', 'gt'])\n",
      "rectified_16_dataset_fog_release\n",
      "torch.Size([15552])\n",
      "tensor([2, 2, 2], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "import joblib, random, torch\n",
    "\n",
    "random.seed(11)\n",
    "pickle_path = \"data/rectified_data/dataset_fog_release/val_dataset_fog_release.p\"\n",
    "\n",
    "all_data = joblib.load(pickle_path)\n",
    "\n",
    "print(len(all_data.keys()))\n",
    "print(all_data[200].keys())\n",
    "\n",
    "gt = all_data[200]['gt']\n",
    "print(all_data[200]['series_name'])\n",
    "print(gt.shape)\n",
    "print(gt[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[1.7707, 2.4249, 0.1701, 0.8715, 2.5939, 1.6950],\n",
       "         [1.1421, 0.6567, 0.1020, 0.8278, 0.0357, 1.0962]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "bce = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "pred = torch.rand(2,6)\n",
    "mask = torch.tensor([[0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 1]]).float()\n",
    "\n",
    "loss = bce(pred, mask)\n",
    "loss.shape, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToObj:\n",
    "    def __init__(self, dictionary):\n",
    "        self.__dict__.update(dictionary)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return self.__dict__.get(name)\n",
    "    \n",
    "opt = {\n",
    "    'seed': 11,\n",
    "    'optimizer': 'adamw',\n",
    "    'learning_rate': 0.00026,\n",
    "    'adam_betas': (0.9, 0.98),\n",
    "    'adam_eps': 1.0e-09,\n",
    "    'weight_decay': 0,\n",
    "    'lr_scheduler_factor': 0.1,\n",
    "    'lr_scheduler_patience': 20,\n",
    "    'lr_scheduler_warmup_steps': 64,\n",
    "    'train_num_steps': 20000,\n",
    "    'penalty_cost': 2.0,\n",
    "    'block_size': 15552,\n",
    "    'block_stride': 972,\n",
    "    'patch_size': 18,\n",
    "    'fog_model_input_dim': 162,\n",
    "    'fog_model_dim': 320,\n",
    "    'fog_model_num_heads': 8,\n",
    "    'fog_model_num_encoder_layers': 5,\n",
    "    'fog_model_num_lstm_layers': 2,\n",
    "    'fog_model_first_dropout': 0.1,\n",
    "    'fog_model_encoder_dropout': 0.1,\n",
    "    'fog_model_mha_dropout': 0.0,\n",
    "}\n",
    "opt = DictToObj(opt)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from models.transformer_bilstm_v1 import TransformerBiLSTM\n",
    "\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "\n",
    "model = TransformerBiLSTM(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from data.fog_dataset_v1 import FoGDataset\n",
    "\n",
    "train_dpath = \"data/rectified_data/dataset_fog_release/train1_dataset_fog_release_blks15552_ps18.p\"\n",
    "model_path = \"runs/train/transfomer_bilstm/2024_06_12_21:12:41.10/weights/model_regular_19991.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path)['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1055,\n",
       " dict_keys(['series_name', 'start_t_idx', 'end_t_idx', 'model_input', 'gt']),\n",
       " torch.Size([864, 162]),\n",
       " torch.Size([864, 3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "train_ds = joblib.load(train_dpath)\n",
    "len(train_ds.keys()), train_ds[1].keys(), train_ds[1]['model_input'].shape, train_ds[1]['gt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 864, 162]), torch.Size([32, 864, 3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = []\n",
    "gt = []\n",
    "for i in range(32):\n",
    "    model_input.append(train_ds[i]['model_input'][None, :, :])\n",
    "    gt.append(train_ds[i]['gt'][None, :, :])\n",
    "\n",
    "model_input= torch.cat(model_input, dim=0)\n",
    "gt = torch.cat(gt, dim=0)\n",
    "model_input.shape, gt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 864, 162]), torch.Size([32, 864, 3]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.fog_dataset_v1 import FoGDataset\n",
    "\n",
    "val_dpath = \"data/rectified_data/dataset_fog_release/val1_dataset_fog_release_blks15552_ps18.p\"\n",
    "model_path = \"runs/train/transfomer_bilstm/2024_06_12_21:12:41.10/weights/model_regular_19991.pt\"\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_path)['model'])\n",
    "import joblib\n",
    "val_ds = joblib.load(val_dpath)\n",
    "\n",
    "model_input = []\n",
    "gt = []\n",
    "for i in range(32):\n",
    "    model_input.append(val_ds[i]['model_input'][None, :, :])\n",
    "    gt.append(val_ds[i]['gt'][None, :, :])\n",
    "\n",
    "model_input= torch.cat(model_input, dim=0)\n",
    "gt = torch.cat(gt, dim=0)\n",
    "\n",
    "model_input.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7070),\n",
       " tensor(0.5953),\n",
       " tensor(0.6464),\n",
       " tensor(0.4415),\n",
       " tensor(4.1332, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(model_input)\n",
    "loss = loss_func(pred, gt)\n",
    "precision, recall, f1, ap = evaluation_metrics(pred, gt)\n",
    "precision, recall, f1, ap, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def norm_axis(a,b,c):\n",
    "    newa=a/(math.sqrt(float(a*a+b*b+c*c)))\n",
    "    newb=b/(math.sqrt(float(a*a+b*b+c*c)))\n",
    "    newc=c/(math.sqrt(float(a*a+b*b+c*c)))\n",
    "    return ([newa,newb,newc])\n",
    "\n",
    "def rotation_matrix(axis, theta):\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis/math.sqrt(np.dot(axis, axis))\n",
    "    a = math.cos(theta/2.0)\n",
    "    b, c, d = -axis*math.sin(theta/2.0)\n",
    "    aa, bb, cc, dd = a*a, b*b, c*c, d*d\n",
    "    bc, ad, ac, ab, bd, cd = b*c, a*d, a*c, a*b, b*d, c*d\n",
    "    return np.array([[aa+bb-cc-dd, 2*(bc+ad), 2*(bd-ac)], \n",
    "                     [2*(bc-ad), aa+cc-bb-dd, 2*(cd+ab)], \n",
    "                     [2*(bd+ac), 2*(cd-ab), aa+dd-bb-cc]])\n",
    "\n",
    "def rotateC(image,theta,a,b,c): ## theta: angle, a, b, c, eular vector\n",
    "    axis=norm_axis(a,b,c)\n",
    "    imagenew=np.dot(image, rotation_matrix(axis,theta))\n",
    "    return imagenew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 864, 162]), torch.Size([32, 864, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.fog_dataset_v1 import FoGDataset\n",
    "\n",
    "train_dpath = \"data/rectified_data/dataset_fog_release/train1_dataset_fog_release_blks15552_ps18.p\"\n",
    "import joblib\n",
    "train_ds = joblib.load(train_dpath)\n",
    "\n",
    "model_input = []\n",
    "gt = []\n",
    "for i in range(32):\n",
    "    model_input.append(train_ds[i]['model_input'][None, :, :])\n",
    "    gt.append(train_ds[i]['gt'][None, :, :])\n",
    "\n",
    "model_input= torch.cat(model_input, dim=0)\n",
    "gt = torch.cat(gt, dim=0)\n",
    "model_input.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 864, 18, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = model_input.reshape(32,864, 18, 9)\n",
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 864, 18, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = model_input.reshape(32, 864, 18, 3, 3)\n",
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3),\n",
       " array([[ 0.38501953, -0.68464825,  0.61888345],\n",
       "        [ 0.91940947,  0.22620226, -0.32174332],\n",
       "        [ 0.08028816,  0.69288477,  0.71656438]]),\n",
       " 215.03446095139176)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "theta = random.random()*math.pi*2\n",
    "theta = random.random()*360\n",
    "a=random.random()\n",
    "b=random.random()\n",
    "c=random.random()\n",
    "axis=norm_axis(a,b,c)\n",
    "tmp = rotation_matrix(axis,theta)\n",
    "tmp.shape, tmp, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.6030, -0.3699, -0.0446],\n",
      "          [-0.6301, -0.0343, -1.0127],\n",
      "          [-0.6837,  0.2872,  0.8251]],\n",
      "\n",
      "         [[-0.6274, -0.3699, -0.0446],\n",
      "          [-0.6479,  0.0098, -0.9907],\n",
      "          [-0.6837,  0.0158,  0.6969]],\n",
      "\n",
      "         [[-0.6274, -0.3699, -0.0446],\n",
      "          [-0.6479,  0.0539, -0.9907],\n",
      "          [-0.7707, -0.0684,  0.6969]],\n",
      "\n",
      "         [[-0.5542, -0.3425, -0.1125],\n",
      "          [-0.6479,  0.0098, -0.9907],\n",
      "          [-0.7707, -0.0684,  0.6969]],\n",
      "\n",
      "         [[-0.5542, -0.3425, -0.1125],\n",
      "          [-0.6301, -0.0343, -0.9443],\n",
      "          [-0.7137,  0.0158,  0.5687]],\n",
      "\n",
      "         [[-0.5542, -0.3972, -0.0786],\n",
      "          [-0.6301,  0.0098, -0.9907],\n",
      "          [-0.7137,  0.0158,  0.5012]],\n",
      "\n",
      "         [[-0.5542, -0.3972, -0.0786],\n",
      "          [-0.6123,  0.0098, -1.0347],\n",
      "          [-0.7437, -0.1619,  0.6294]],\n",
      "\n",
      "         [[-0.5786, -0.3972, -0.0786],\n",
      "          [-0.6301, -0.0829, -0.9907],\n",
      "          [-0.7437, -0.2555,  0.7576]],\n",
      "\n",
      "         [[-0.5786, -0.3972, -0.0786],\n",
      "          [-0.6301, -0.0343, -1.0127],\n",
      "          [-0.7437, -0.1619,  0.5687]],\n",
      "\n",
      "         [[-0.6518, -0.3425, -0.0786],\n",
      "          [-0.6479,  0.0098, -0.9907],\n",
      "          [-0.7707, -0.0684,  0.6294]],\n",
      "\n",
      "         [[-0.6518, -0.3425, -0.0786],\n",
      "          [-0.6301, -0.1270, -1.0127],\n",
      "          [-0.7137, -0.0684,  0.6294]],\n",
      "\n",
      "         [[-0.6030, -0.3425, -0.0446],\n",
      "          [-0.6123, -0.0829, -0.9663],\n",
      "          [-0.7137, -0.3397,  0.5687]],\n",
      "\n",
      "         [[-0.6030, -0.3425, -0.0446],\n",
      "          [-0.6123, -0.0829, -0.9663],\n",
      "          [-0.8006, -0.1619,  0.6969]],\n",
      "\n",
      "         [[-0.6030, -0.4245, -0.0446],\n",
      "          [-0.6479, -0.0829, -0.9663],\n",
      "          [-0.7707, -0.2555,  0.6969]],\n",
      "\n",
      "         [[-0.6030, -0.4245, -0.0446],\n",
      "          [-0.6479, -0.0343, -0.9443],\n",
      "          [-0.8006, -0.1619,  0.6294]],\n",
      "\n",
      "         [[-0.6030, -0.3425, -0.0786],\n",
      "          [-0.6301, -0.0829, -0.9907],\n",
      "          [-0.8306, -0.0684,  0.5687]],\n",
      "\n",
      "         [[-0.6030, -0.3425, -0.0786],\n",
      "          [-0.6301, -0.1270, -0.9443],\n",
      "          [-0.8306, -0.0684,  0.5687]],\n",
      "\n",
      "         [[-0.6030, -0.3699, -0.0786],\n",
      "          [-0.6301,  0.0098, -0.9907],\n",
      "          [-0.7707, -0.1619,  0.6294]]],\n",
      "\n",
      "\n",
      "        [[[-0.5786, -0.3425, -0.1125],\n",
      "          [-0.6301, -0.0829, -0.9663],\n",
      "          [-0.7437, -0.2555,  0.5687]],\n",
      "\n",
      "         [[-0.5786, -0.3425, -0.1125],\n",
      "          [-0.6479, -0.0829, -0.9907],\n",
      "          [-0.7437, -0.1619,  0.6969]],\n",
      "\n",
      "         [[-0.6030, -0.3699, -0.0786],\n",
      "          [-0.6479,  0.0098, -0.9663],\n",
      "          [-0.7707, -0.4333,  0.6969]],\n",
      "\n",
      "         [[-0.5542, -0.3972, -0.0786],\n",
      "          [-0.6301,  0.0098, -0.9443],\n",
      "          [-0.7437, -0.3397,  0.5687]],\n",
      "\n",
      "         [[-0.5542, -0.3972, -0.0786],\n",
      "          [-0.6301, -0.0343, -1.0127],\n",
      "          [-0.7707, -0.0684,  0.5012]],\n",
      "\n",
      "         [[-0.6030, -0.3425, -0.0446],\n",
      "          [-0.6479,  0.0098, -0.9663],\n",
      "          [-0.7707, -0.3397,  0.7576]],\n",
      "\n",
      "         [[-0.6030, -0.3425, -0.0446],\n",
      "          [-0.6479, -0.0343, -0.9663],\n",
      "          [-0.7707, -0.1619,  0.5687]],\n",
      "\n",
      "         [[-0.6030, -0.4245, -0.0446],\n",
      "          [-0.6301,  0.0539, -0.9907],\n",
      "          [-0.8306,  0.0158,  0.6294]],\n",
      "\n",
      "         [[-0.6030, -0.3699, -0.0786],\n",
      "          [-0.6479,  0.0098, -0.9907],\n",
      "          [-0.7707, -0.1619,  0.7576]],\n",
      "\n",
      "         [[-0.6030, -0.3699, -0.0786],\n",
      "          [-0.6479,  0.0098, -0.9907],\n",
      "          [-0.8006, -0.1619,  0.5687]],\n",
      "\n",
      "         [[-0.5786, -0.3425, -0.0786],\n",
      "          [-0.6123,  0.0539, -1.0347],\n",
      "          [-0.8006, -0.1619,  0.6294]],\n",
      "\n",
      "         [[-0.6030, -0.3699, -0.1125],\n",
      "          [-0.6123, -0.1270, -0.9907],\n",
      "          [-0.7707, -0.3397,  0.6969]],\n",
      "\n",
      "         [[-0.6030, -0.3699, -0.1125],\n",
      "          [-0.6657, -0.0829, -1.0127],\n",
      "          [-0.7707, -0.3397,  0.6294]],\n",
      "\n",
      "         [[-0.6030, -0.3425, -0.1125],\n",
      "          [-0.6479,  0.0539, -1.0127],\n",
      "          [-0.8006, -0.3397,  0.6969]],\n",
      "\n",
      "         [[-0.6030, -0.4792, -0.0106],\n",
      "          [-0.6479, -0.0343, -1.0127],\n",
      "          [-0.7707, -0.2555,  0.6294]],\n",
      "\n",
      "         [[-0.6030, -0.4792, -0.0106],\n",
      "          [-0.6301,  0.0539, -0.9907],\n",
      "          [-0.7707, -0.3397,  0.6294]],\n",
      "\n",
      "         [[-0.5786, -0.3425, -0.0786],\n",
      "          [-0.6479,  0.0539, -0.9907],\n",
      "          [-0.7707, -0.3397,  0.6294]],\n",
      "\n",
      "         [[-0.5786, -0.3425, -0.0786],\n",
      "          [-0.6301, -0.0343, -0.9443],\n",
      "          [-0.7707, -0.1619,  0.7576]]]])\n",
      "\n",
      "[[[[-0.65867335 -0.25773804 -0.04611335]\n",
      "   [-0.47125663 -0.3060434  -1.05258558]\n",
      "   [-0.71052971  0.67903355  0.51455767]]\n",
      "\n",
      "  [[-0.68220213 -0.25380784 -0.05134019]\n",
      "   [-0.48149679 -0.25466207 -1.05100004]\n",
      "   [-0.75398781  0.38329243  0.48781002]]\n",
      "\n",
      "  [[-0.68220213 -0.25380784 -0.05134019]\n",
      "   [-0.47147285 -0.21430918 -1.06578071]\n",
      "   [-0.85687631  0.32027554  0.49741002]]\n",
      "\n",
      "  [[-0.59577262 -0.26582648 -0.10714944]\n",
      "   [-0.48149679 -0.25466207 -1.05100004]\n",
      "   [-0.85687631  0.32027554  0.49741002]]\n",
      "\n",
      "  [[-0.59577262 -0.26582648 -0.10714944]\n",
      "   [-0.4809573  -0.28064303 -0.9898185 ]\n",
      "   [-0.76468955  0.34051223  0.36375752]]\n",
      "\n",
      "  [[-0.61300832 -0.30320794 -0.05767051]\n",
      "   [-0.46435076 -0.25752611 -1.04719111]\n",
      "   [-0.75512075  0.31545717  0.30184372]]\n",
      "\n",
      "  [[-0.61300832 -0.30320794 -0.05767051]\n",
      "   [-0.44096859 -0.27671895 -1.08373245]\n",
      "   [-0.84256807  0.20531414  0.47261163]]\n",
      "\n",
      "  [[-0.63653715 -0.29927773 -0.06289737]\n",
      "   [-0.48540104 -0.34226717 -1.01615171]\n",
      "   [-0.88200358  0.16735452  0.62158891]]\n",
      "\n",
      "  [[-0.63653715 -0.29927773 -0.06289737]\n",
      "   [-0.47125663 -0.3060434  -1.05258558]\n",
      "   [-0.83395614  0.18276455  0.41688913]]\n",
      "\n",
      "  [[-0.69470468 -0.23749316 -0.09688995]\n",
      "   [-0.48149679 -0.25466207 -1.05100004]\n",
      "   [-0.84730751  0.29522046  0.43549617]]\n",
      "\n",
      "  [[-0.69470468 -0.23749316 -0.09688995]\n",
      "   [-0.4923069  -0.39078447 -1.02154618]\n",
      "   [-0.79243077  0.28605396  0.44768685]]\n",
      "\n",
      "  [[-0.65246393 -0.23274106 -0.05526937]\n",
      "   [-0.47171953 -0.33605965 -0.98992597]\n",
      "   [-0.84545767  0.01536794  0.48285305]]\n",
      "\n",
      "  [[-0.65246393 -0.23274106 -0.05526937]\n",
      "   [-0.47171953 -0.33605965 -0.98992597]\n",
      "   [-0.90701356  0.23953572  0.52233481]]\n",
      "\n",
      "  [[-0.67109221 -0.30773201 -0.0278013 ]\n",
      "   [-0.50601159 -0.33033158 -0.99754384]\n",
      "   [-0.89938586  0.14914695  0.56009189]]\n",
      "\n",
      "  [[-0.67109221 -0.30773201 -0.0278013 ]\n",
      "   [-0.49810333 -0.277779   -0.99362744]\n",
      "   [-0.89744475  0.21448063  0.46042095]]\n",
      "\n",
      "  [[-0.64764707 -0.24535356 -0.08643626]\n",
      "   [-0.48540104 -0.34226717 -1.01615171]\n",
      "   [-0.89646058  0.28231982  0.36694137]]\n",
      "\n",
      "  [[-0.64764707 -0.24535356 -0.08643626]\n",
      "   [-0.50200758 -0.3653841  -0.95877911]\n",
      "   [-0.89646058  0.28231982  0.36694137]]\n",
      "\n",
      "  [[-0.6538565  -0.27035055 -0.07728023]\n",
      "   [-0.46435076 -0.25752611 -1.04719111]\n",
      "   [-0.86856228  0.20965616  0.4668371 ]]]\n",
      "\n",
      "\n",
      " [[[-0.61930145 -0.26189627 -0.11237629]\n",
      "   [-0.48886556 -0.33319561 -0.99373491]\n",
      "   [-0.85521092  0.09720026  0.44823007]]\n",
      "\n",
      "  [[-0.61930145 -0.26189627 -0.11237629]\n",
      "   [-0.50254706 -0.33940313 -1.01996065]\n",
      "   [-0.85213688  0.23036923  0.53452548]]\n",
      "\n",
      "  [[-0.6538565  -0.27035055 -0.07728023]\n",
      "   [-0.48496131 -0.24559051 -1.02858324]\n",
      "   [-0.93976992 -0.0134252   0.61963966]]\n",
      "\n",
      "  [[-0.61300832 -0.30320794 -0.05767051]\n",
      "   [-0.47093336 -0.24029015 -1.00459917]\n",
      "   [-0.8743402   0.02019242  0.4764369 ]]\n",
      "\n",
      "  [[-0.61300832 -0.30320794 -0.05767051]\n",
      "   [-0.47125663 -0.3060434  -1.05258558]\n",
      "   [-0.82912678  0.24761581  0.31785987]]\n",
      "\n",
      "  [[-0.65246393 -0.23274106 -0.05526937]\n",
      "   [-0.48496131 -0.24559051 -1.02858324]\n",
      "   [-0.92712707  0.0946887   0.64402121]]\n",
      "\n",
      "  [[-0.65246393 -0.23274106 -0.05526937]\n",
      "   [-0.49498525 -0.2859434  -1.01380257]\n",
      "   [-0.85995035  0.18710657  0.41111461]]\n",
      "\n",
      "  [[-0.67109221 -0.30773201 -0.0278013 ]\n",
      "   [-0.45432682 -0.21717322 -1.06197177]\n",
      "   [-0.88594322  0.38187727  0.39445702]]\n",
      "\n",
      "  [[-0.6538565  -0.27035055 -0.07728023]\n",
      "   [-0.48149679 -0.25466207 -1.05100004]\n",
      "   [-0.88674301  0.25726083  0.58447345]]\n",
      "\n",
      "  [[-0.6538565  -0.27035055 -0.07728023]\n",
      "   [-0.48149679 -0.25466207 -1.05100004]\n",
      "   [-0.88883282  0.19193104  0.40469846]]\n",
      "\n",
      "  [[-0.6241183  -0.24928376 -0.08120942]\n",
      "   [-0.43094465 -0.23636607 -1.09851311]\n",
      "   [-0.89744475  0.21448063  0.46042095]]\n",
      "\n",
      "  [[-0.64903965 -0.28296305 -0.10844711]\n",
      "   [-0.47827895 -0.3854841  -0.99756211]\n",
      "   [-0.91851514  0.07213911  0.58829872]]\n",
      "\n",
      "  [[-0.64903965 -0.28296305 -0.10844711]\n",
      "   [-0.51657501 -0.3447035  -1.04394471]\n",
      "   [-0.90894634  0.04708402  0.52638486]]\n",
      "\n",
      "  [[-0.64283022 -0.25796607 -0.11760313]\n",
      "   [-0.46835477 -0.22247359 -1.08595584]\n",
      "   [-0.94739762  0.07696358  0.58188257]]\n",
      "\n",
      "  [[-0.68832791 -0.34511348  0.02167763]\n",
      "   [-0.48840265 -0.30317936 -1.05639451]\n",
      "   [-0.88981705  0.12409186  0.49817803]]\n",
      "\n",
      "  [[-0.68832791 -0.34511348  0.02167763]\n",
      "   [-0.45432682 -0.21717322 -1.06197177]\n",
      "   [-0.90894634  0.04708402  0.52638486]]\n",
      "\n",
      "  [[-0.6241183  -0.24928376 -0.08120942]\n",
      "   [-0.47147285 -0.21430918 -1.06578071]\n",
      "   [-0.90894634  0.04708402  0.52638486]]\n",
      "\n",
      "  [[-0.6241183  -0.24928376 -0.08120942]\n",
      "   [-0.4809573  -0.28064303 -0.9898185 ]\n",
      "   [-0.88674301  0.25726083  0.58447345]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 864, 18, 3, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(11)\n",
    "np.random.seed(11)\n",
    "torch.manual_seed(11)\n",
    "\n",
    "theta = random.random()*math.pi*2\n",
    "theta = random.random()*360\n",
    "a=random.random()\n",
    "b=random.random()\n",
    "c=random.random()\n",
    "\n",
    "print(model_input[0,:2,:,:])\n",
    "print()\n",
    "tmp=rotateC(model_input.cpu().detach().numpy(),theta,a,b,c)\n",
    "print(tmp[0,:2,:,:])\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ratio of two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4, 2, 6],\n",
       "         [8, 3, 1],\n",
       "         [3, 7, 4],\n",
       "         [8, 1, 5]], device='cuda:3'),\n",
       " tensor([[4, 8, 3, 8],\n",
       "         [2, 3, 7, 1],\n",
       "         [6, 1, 4, 5]], device='cuda:3'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a= torch.randint(0,9, size=(4,3)).to(\"cuda:3\")\n",
    "a, a.permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23288"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, torch\n",
    "\n",
    "all_dpath = \"data/rectified_data/kaggle_pd_data/train_kaggle_pd_data_blks15552_ps18_randomaug.p\"\n",
    "all_data = joblib.load(all_dpath)\n",
    "len(all_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2402, 2402)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for key, value in all_data.items():\n",
    "    if value['series_name'] in a:\n",
    "        count += 1\n",
    "count, len(all_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([864, 3]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [1., 0., 0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[3]['gt'].shape, all_data[3]['gt'][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one:  2059578\n",
      "zero:  4901310\n",
      "two:  13154766\n",
      "zero to one:  2.379764204123369\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the ratio\n",
    "def calculate_class_ratio(train_data):\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "    twos = 0\n",
    "    for example_id, example_data in train_data.items():\n",
    "        gt = example_data['gt']\n",
    "        num_zeros = torch.sum(gt[:,0] == 1).item()\n",
    "        num_ones = torch.sum(gt[:,1] == 1).item()\n",
    "        num_twos = torch.sum(gt[:,2] == 1).item()\n",
    "        ones += num_ones\n",
    "        zeros += num_zeros\n",
    "        twos += num_twos\n",
    "    print(\"one: \", ones)\n",
    "    print(\"zero: \", zeros)\n",
    "    print(\"two: \", twos)\n",
    "    print(\"zero to one: \", zeros / ones)\n",
    "\n",
    "# Calculate the ratio for the training data\n",
    "calculate_class_ratio(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3, -9],\n",
       "         [ 5, -9],\n",
       "         [ 6, -9]]),\n",
       " tensor([[3, 8],\n",
       "         [5, 2],\n",
       "         [6, 8]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randint(0, 9, (3, 2))\n",
    "b = a.clone()\n",
    "a[:, 1] = -9\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125387    2.0\n",
      "125388    2.0\n",
      "125389    2.0\n",
      "Name: GroundTruth_Trial44, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path and the column name\n",
    "file_path = 'data/rectified_data/kaggle_pd_data/defog/gt_kaggle_pd_data.csv'\n",
    "column_name = 'GroundTruth_Trial44'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the specific column\n",
    "column_data = df[column_name]\n",
    "\n",
    "# Print rows 10 to 25 (index 9 to 24) of the column\n",
    "print(column_data.iloc[125387:125390])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
