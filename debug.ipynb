{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# device0 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device7 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "OCCUPY_CUDA_7 = torch.randn(1, 1, device=device7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'GeneralEvent', 'ClinicalEvent', 'L Foot Contact', 'R Foot Contact', 'L Foot Pressure', 'R Foot Pressure', 'Walkway_X', 'Walkway_Y', 'WalkwayPressureLevel', 'WalkwayFoot', 'LowerBack_Acc_X', 'LowerBack_Acc_Y', 'LowerBack_Acc_Z', 'LowerBack_FreeAcc_E', 'LowerBack_FreeAcc_N', 'LowerBack_FreeAcc_U', 'LowerBack_Gyr_X', 'LowerBack_Gyr_Y', 'LowerBack_Gyr_Z', 'LowerBack_Mag_X', 'LowerBack_Mag_Y', 'LowerBack_Mag_Z', 'LowerBack_VelInc_X', 'LowerBack_VelInc_Y', 'LowerBack_VelInc_Z', 'LowerBack_OriInc_q0', 'LowerBack_OriInc_q1', 'LowerBack_OriInc_q2', 'LowerBack_OriInc_q3', 'LowerBack_Roll', 'LowerBack_Pitch', 'LowerBack_Yaw', 'R_Wrist_Acc_X', 'R_Wrist_Acc_Y', 'R_Wrist_Acc_Z', 'R_Wrist_FreeAcc_E', 'R_Wrist_FreeAcc_N', 'R_Wrist_FreeAcc_U', 'R_Wrist_Gyr_X', 'R_Wrist_Gyr_Y', 'R_Wrist_Gyr_Z', 'R_Wrist_Mag_X', 'R_Wrist_Mag_Y', 'R_Wrist_Mag_Z', 'R_Wrist_VelInc_X', 'R_Wrist_VelInc_Y', 'R_Wrist_VelInc_Z', 'R_Wrist_OriInc_q0', 'R_Wrist_OriInc_q1', 'R_Wrist_OriInc_q2', 'R_Wrist_OriInc_q3', 'R_Wrist_Roll', 'R_Wrist_Pitch', 'R_Wrist_Yaw', 'L_Wrist_Acc_X', 'L_Wrist_Acc_Y', 'L_Wrist_Acc_Z', 'L_Wrist_FreeAcc_E', 'L_Wrist_FreeAcc_N', 'L_Wrist_FreeAcc_U', 'L_Wrist_Gyr_X', 'L_Wrist_Gyr_Y', 'L_Wrist_Gyr_Z', 'L_Wrist_Mag_X', 'L_Wrist_Mag_Y', 'L_Wrist_Mag_Z', 'L_Wrist_VelInc_X', 'L_Wrist_VelInc_Y', 'L_Wrist_VelInc_Z', 'L_Wrist_OriInc_q0', 'L_Wrist_OriInc_q1', 'L_Wrist_OriInc_q2', 'L_Wrist_OriInc_q3', 'L_Wrist_Roll', 'L_Wrist_Pitch', 'L_Wrist_Yaw', 'R_MidLatThigh_Acc_X', 'R_MidLatThigh_Acc_Y', 'R_MidLatThigh_Acc_Z', 'R_MidLatThigh_FreeAcc_E', 'R_MidLatThigh_FreeAcc_N', 'R_MidLatThigh_FreeAcc_U', 'R_MidLatThigh_Gyr_X', 'R_MidLatThigh_Gyr_Y', 'R_MidLatThigh_Gyr_Z', 'R_MidLatThigh_Mag_X', 'R_MidLatThigh_Mag_Y', 'R_MidLatThigh_Mag_Z', 'R_MidLatThigh_VelInc_X', 'R_MidLatThigh_VelInc_Y', 'R_MidLatThigh_VelInc_Z', 'R_MidLatThigh_OriInc_q0', 'R_MidLatThigh_OriInc_q1', 'R_MidLatThigh_OriInc_q2', 'R_MidLatThigh_OriInc_q3', 'R_MidLatThigh_Roll', 'R_MidLatThigh_Pitch', 'R_MidLatThigh_Yaw', 'L_MidLatThigh_Acc_X', 'L_MidLatThigh_Acc_Y', 'L_MidLatThigh_Acc_Z', 'L_MidLatThigh_FreeAcc_E', 'L_MidLatThigh_FreeAcc_N', 'L_MidLatThigh_FreeAcc_U', 'L_MidLatThigh_Gyr_X', 'L_MidLatThigh_Gyr_Y', 'L_MidLatThigh_Gyr_Z', 'L_MidLatThigh_Mag_X', 'L_MidLatThigh_Mag_Y', 'L_MidLatThigh_Mag_Z', 'L_MidLatThigh_VelInc_X', 'L_MidLatThigh_VelInc_Y', 'L_MidLatThigh_VelInc_Z', 'L_MidLatThigh_OriInc_q0', 'L_MidLatThigh_OriInc_q1', 'L_MidLatThigh_OriInc_q2', 'L_MidLatThigh_OriInc_q3', 'L_MidLatThigh_Roll', 'L_MidLatThigh_Pitch', 'L_MidLatThigh_Yaw', 'R_LatShank_Acc_X', 'R_LatShank_Acc_Y', 'R_LatShank_Acc_Z', 'R_LatShank_FreeAcc_E', 'R_LatShank_FreeAcc_N', 'R_LatShank_FreeAcc_U', 'R_LatShank_Gyr_X', 'R_LatShank_Gyr_Y', 'R_LatShank_Gyr_Z', 'R_LatShank_Mag_X', 'R_LatShank_Mag_Y', 'R_LatShank_Mag_Z', 'R_LatShank_VelInc_X', 'R_LatShank_VelInc_Y', 'R_LatShank_VelInc_Z', 'R_LatShank_OriInc_q0', 'R_LatShank_OriInc_q1', 'R_LatShank_OriInc_q2', 'R_LatShank_OriInc_q3', 'R_LatShank_Roll', 'R_LatShank_Pitch', 'R_LatShank_Yaw', 'L_LatShank_Acc_X', 'L_LatShank_Acc_Y', 'L_LatShank_Acc_Z', 'L_LatShank_FreeAcc_E', 'L_LatShank_FreeAcc_N', 'L_LatShank_FreeAcc_U', 'L_LatShank_Gyr_X', 'L_LatShank_Gyr_Y', 'L_LatShank_Gyr_Z', 'L_LatShank_Mag_X', 'L_LatShank_Mag_Y', 'L_LatShank_Mag_Z', 'L_LatShank_VelInc_X', 'L_LatShank_VelInc_Y', 'L_LatShank_VelInc_Z', 'L_LatShank_OriInc_q0', 'L_LatShank_OriInc_q1', 'L_LatShank_OriInc_q2', 'L_LatShank_OriInc_q3', 'L_LatShank_Roll', 'L_LatShank_Pitch', 'L_LatShank_Yaw', 'R_DorsalFoot_Acc_X', 'R_DorsalFoot_Acc_Y', 'R_DorsalFoot_Acc_Z', 'R_DorsalFoot_FreeAcc_E', 'R_DorsalFoot_FreeAcc_N', 'R_DorsalFoot_FreeAcc_U', 'R_DorsalFoot_Gyr_X', 'R_DorsalFoot_Gyr_Y', 'R_DorsalFoot_Gyr_Z', 'R_DorsalFoot_Mag_X', 'R_DorsalFoot_Mag_Y', 'R_DorsalFoot_Mag_Z', 'R_DorsalFoot_VelInc_X', 'R_DorsalFoot_VelInc_Y', 'R_DorsalFoot_VelInc_Z', 'R_DorsalFoot_OriInc_q0', 'R_DorsalFoot_OriInc_q1', 'R_DorsalFoot_OriInc_q2', 'R_DorsalFoot_OriInc_q3', 'R_DorsalFoot_Roll', 'R_DorsalFoot_Pitch', 'R_DorsalFoot_Yaw', 'L_DorsalFoot_Acc_X', 'L_DorsalFoot_Acc_Y', 'L_DorsalFoot_Acc_Z', 'L_DorsalFoot_FreeAcc_E', 'L_DorsalFoot_FreeAcc_N', 'L_DorsalFoot_FreeAcc_U', 'L_DorsalFoot_Gyr_X', 'L_DorsalFoot_Gyr_Y', 'L_DorsalFoot_Gyr_Z', 'L_DorsalFoot_Mag_X', 'L_DorsalFoot_Mag_Y', 'L_DorsalFoot_Mag_Z', 'L_DorsalFoot_VelInc_X', 'L_DorsalFoot_VelInc_Y', 'L_DorsalFoot_VelInc_Z', 'L_DorsalFoot_OriInc_q0', 'L_DorsalFoot_OriInc_q1', 'L_DorsalFoot_OriInc_q2', 'L_DorsalFoot_OriInc_q3', 'L_DorsalFoot_Roll', 'L_DorsalFoot_Pitch', 'L_DorsalFoot_Yaw', 'R_Ankle_Acc_X', 'R_Ankle_Acc_Y', 'R_Ankle_Acc_Z', 'R_Ankle_FreeAcc_E', 'R_Ankle_FreeAcc_N', 'R_Ankle_FreeAcc_U', 'R_Ankle_Gyr_X', 'R_Ankle_Gyr_Y', 'R_Ankle_Gyr_Z', 'R_Ankle_Mag_X', 'R_Ankle_Mag_Y', 'R_Ankle_Mag_Z', 'R_Ankle_VelInc_X', 'R_Ankle_VelInc_Y', 'R_Ankle_VelInc_Z', 'R_Ankle_OriInc_q0', 'R_Ankle_OriInc_q1', 'R_Ankle_OriInc_q2', 'R_Ankle_OriInc_q3', 'R_Ankle_Roll', 'R_Ankle_Pitch', 'R_Ankle_Yaw', 'L_Ankle_Acc_X', 'L_Ankle_Acc_Y', 'L_Ankle_Acc_Z', 'L_Ankle_FreeAcc_E', 'L_Ankle_FreeAcc_N', 'L_Ankle_FreeAcc_U', 'L_Ankle_Gyr_X', 'L_Ankle_Gyr_Y', 'L_Ankle_Gyr_Z', 'L_Ankle_Mag_X', 'L_Ankle_Mag_Y', 'L_Ankle_Mag_Z', 'L_Ankle_VelInc_X', 'L_Ankle_VelInc_Y', 'L_Ankle_VelInc_Z', 'L_Ankle_OriInc_q0', 'L_Ankle_OriInc_q1', 'L_Ankle_OriInc_q2', 'L_Ankle_OriInc_q3', 'L_Ankle_Roll', 'L_Ankle_Pitch', 'L_Ankle_Yaw', 'Xiphoid_Acc_X', 'Xiphoid_Acc_Y', 'Xiphoid_Acc_Z', 'Xiphoid_FreeAcc_E', 'Xiphoid_FreeAcc_N', 'Xiphoid_FreeAcc_U', 'Xiphoid_Gyr_X', 'Xiphoid_Gyr_Y', 'Xiphoid_Gyr_Z', 'Xiphoid_Mag_X', 'Xiphoid_Mag_Y', 'Xiphoid_Mag_Z', 'Xiphoid_VelInc_X', 'Xiphoid_VelInc_Y', 'Xiphoid_VelInc_Z', 'Xiphoid_OriInc_q0', 'Xiphoid_OriInc_q1', 'Xiphoid_OriInc_q2', 'Xiphoid_OriInc_q3', 'Xiphoid_Roll', 'Xiphoid_Pitch', 'Xiphoid_Yaw', 'Forehead_Acc_X', 'Forehead_Acc_Y', 'Forehead_Acc_Z', 'Forehead_FreeAcc_E', 'Forehead_FreeAcc_N', 'Forehead_FreeAcc_U', 'Forehead_Gyr_X', 'Forehead_Gyr_Y', 'Forehead_Gyr_Z', 'Forehead_Mag_X', 'Forehead_Mag_Y', 'Forehead_Mag_Z', 'Forehead_VelInc_X', 'Forehead_VelInc_Y', 'Forehead_VelInc_Z', 'Forehead_OriInc_q0', 'Forehead_OriInc_q1', 'Forehead_OriInc_q2', 'Forehead_OriInc_q3', 'Forehead_Roll', 'Forehead_Pitch', 'Forehead_Yaw', 'LPressure1', 'LPressure2', 'LPressure3', 'LPressure4', 'LPressure5', 'LPressure6', 'LPressure7', 'LPressure8', 'LPressure9', 'LPressure10', 'LPressure11', 'LPressure12', 'LPressure13', 'LPressure14', 'LPressure15', 'LPressure16', 'Linsole:Acc_X', 'Linsole:Acc_Y', 'Linsole:Acc_Z', 'Linsole:Gyr_X', 'Linsole:Gyr_Y', 'Linsole:Gyr_Z', 'LTotalForce', 'LCoP_X', 'LCoP_Y', 'RPressure1', 'RPressure2', 'RPressure3', 'RPressure4', 'RPressure5', 'RPressure6', 'RPressure7', 'RPressure8', 'RPressure9', 'RPressure10', 'RPressure11', 'RPressure12', 'RPressure13', 'RPressure14', 'RPressure15', 'RPressure16', 'Rinsole:Acc_X', 'Rinsole:Acc_Y', 'Rinsole:Acc_Z', 'Rinsole:Gyr_X', 'Rinsole:Gyr_Y', 'Rinsole:Gyr_Z', 'RTotalForce', 'RCoP_X', 'RCoP_Y']\n",
      "['0 sec', 'Standing', 'unlabeled', '0', '0', '0', '0', '', '', '', '', '9.507711', '0.776327', '2.311817', '0.064093', '-0.003629', '0.002588', '0.024763', '-0.006137', '0.007981', '-0.307861', '-0.207031', '-0.577393', '0.095076', '0.007764', '0.023122', '1', '0.000124', '-3.1e-05', '4e-05', '18.85588', '-75.248757', '7.896185', '7.540472', '-6.22653', '-1.025152', '0.025394', '0.132122', '0.018957', '0.016273', '-0.013222', '-0.001792', '-0.196533', '0.028809', '0.602783', '0.075405', '-0.062265', '-0.010252', '1', '8.1e-05', '-6.6e-05', '-9e-06', '-100.474532', '-49.778978', '11.611477', '6.948482', '6.592564', '-2.007346', '-0.074251', '-0.028408', '-0.026655', '0.061071', '0.021176', '-0.011099', '-0.144043', '-0.004395', '0.626953', '0.069486', '0.065928', '-0.020061', '1', '0.000305', '0.000106', '-5.5e-05', '107.59071', '-45.131997', '-82.344035', '9.656365', '1.595453', '-0.686904', '0.091219', '-0.059355', '-0.001936', '0.028465', '-0.007651', '-0.002244', '-0.193604', '-0.782471', '0.060303', '0.096564', '0.015954', '-0.006863', '1', '0.000142', '-3.8e-05', '-1.1e-05', '113.997116', '-79.18278', '-22.023301', '9.757522', '0.834896', '-0.023661', '-0.025359', '-0.035821', '-0.019581', '0.012082', '0.006454', '-0.025327', '-0.351074', '0.62915', '0.151123', '0.097576', '0.008337', '-0.000239', '1', '6e-05', '3.2e-05', '-0.000127', '89.179625', '-84.953744', '-178.836204', '9.628705', '1.840344', '0.42421', '-0.079669', '-0.052254', '-0.000974', '0.026474', '-0.010853', '0.020908', '-0.210938', '-0.956787', '0.056396', '0.096285', '0.018413', '0.00425', '1', '0.000132', '-5.4e-05', '0.000105', '79.672064', '-78.693485', '-80.727648', '9.627484', '-1.153264', '1.22416', '-0.028038', '-0.017946', '-0.039462', '0.010434', '0.005232', '-0.018021', '-0.318604', '0.900146', '0.148682', '0.096274', '-0.011542', '0.012238', '1', '5.2e-05', '2.6e-05', '-9e-05', '-42.145862', '-80.066897', '-64.949078', '5.636641', '-0.315958', '8.067635', '-0.043064', '-0.084646', '0.033594', '0.005389', '0.006954', '0.008038', '-1.17749', '-0.659424', '-0.314453', '0.056369', '-0.003159', '0.080674', '1', '2.7e-05', '3.5e-05', '4e-05', '-2.907412', '-34.83884', '161.390232', '6.000921', '0.873217', '7.737417', '0.033725', '-0.034528', '0.017819', '-0.002511', '0.007713', '-0.004263', '-1.281494', '0.110596', '0.13916', '0.060012', '0.008732', '0.077372', '1', '-1.3e-05', '3.9e-05', '-2.1e-05', '6.236819', '-37.388543', '-80.083692', '9.741593', '1.240348', '-0.336869', '-0.008071', '0.018104', '0.013308', '0.010015', '-0.015486', '0.004636', '-0.632813', '-1.139404', '-0.446289', '0.097416', '0.012406', '-0.003361', '1', '5e-05', '-7.7e-05', '2.3e-05', '104.36216', '-82.497669', '18.716164', '9.776784', '-0.529142', '-0.494585', '-0.055976', '0.046991', '-0.009384', '-0.002946', '0.01063', '-0.019836', '-0.4729', '1.135254', '0.013672', '0.097767', '-0.005301', '-0.004951', '1', '-1.5e-05', '5.3e-05', '-9.9e-05', '-129.295578', '-86.099237', '-80.770379', '9.895042', '-0.709164', '-0.158969', '0.035435', '-0.075048', '0.108662', '0.029444', '-0.008846', '0.004141', '-0.295898', '-0.010986', '0.467285', '0.098951', '-0.007089', '-0.001586', '1', '0.000147', '-4.4e-05', '2.1e-05', '-99.253489', '-85.397755', '-33.441631', '-3.108866', '-9.282574', '1.033171', '-0.033781', '0.049075', '0.030845', '-0.014697', '-0.012196', '0.011027', '-0.053711', '0.418457', '0.241455', '-0.031084', '-0.092827', '0.010337', '1', '-7.3e-05', '-6.1e-05', '5.5e-05', '-83.333853', '18.244398', '-116.176913', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "sample_path = 'reference_data/sample_pfda/ValidationDataset-sampledata.csv'\n",
    "rectified_path = 'data/rectified_dataset_fog_release.csv'\n",
    "with open(sample_path, mode ='r')as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    count = 0\n",
    "    for lines in csvFile:\n",
    "        if count == 2:\n",
    "                break\n",
    "        print(lines)\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec_1.csv', 'rec_2.csv', 'rec_11.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rec_1.csv', 'rec_2.csv', 'rec_11.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.listdir('data')\n",
    "rectified_csv_files = [file for file in data_folder if file.startswith('rec') \\\n",
    "                                                        and file.endswith('.csv')]\n",
    "print(rectified_csv_files)\n",
    "rectified_csv_files = sorted(rectified_csv_files, key=lambda x: (int(x[:-4].split('_')[1])))\n",
    "rectified_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['header1', 0, 2, 1, 4], ['header2', 3, 5, 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('header1', 'header2'), (0, 3), (2, 5), (1, 6)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ['header1', 0, 2, 1, 4],\n",
    "    ['header2', 3, 5, 6]\n",
    "]\n",
    "print(data)\n",
    "transposed_data = list(zip(*data))\n",
    "transposed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'AccV', 'AccML', 'AccAP', 'StartHesitation', 'Turn', 'Walking'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"data/kaggle_pd_data/train/tdcsfog/0df04e8431.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.columns)\n",
    "# Count non-zero elements in the specified column\n",
    "non_zero_count = (df[\"Turn\"] != 0).sum()\n",
    "non_zero_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15552, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(10, size=(50000,3))\n",
    "def get_blocks(series):\n",
    "    series = series.clone()\n",
    "    # series = series[columns]\n",
    "    \n",
    "    block_count = math.ceil(series.shape[0] / CFG['block_size'])\n",
    "    \n",
    "    padding = block_count * CFG['block_size'] - series.shape[0]\n",
    "    padding = torch.zeros(size=(padding, series.shape[1]))\n",
    "\n",
    "    series = torch.cat([series, padding], dim=0)\n",
    "    \n",
    "    block_begins = list(range(0, series.shape[0], CFG['block_stride']))\n",
    "    block_begins = [x for x in block_begins if x + CFG['block_size'] <= series.shape[0]]\n",
    "    \n",
    "    blocks = []\n",
    "    for begin in block_begins:\n",
    "        values = series[begin:begin + CFG['block_size']]\n",
    "        blocks.append({'begin': begin,\n",
    "                       'end': begin + CFG['block_size'],\n",
    "                       'values': values})\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "blocks = get_blocks(a)\n",
    "blocks[0]['values'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "device7 = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "OCCUPY_CUDA_7 = torch.randn(1, 1, device=device7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "print(OCCUPY_CUDA_7.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'block_size': 15552, \n",
    "    'block_stride': 15552//16, # 972\n",
    "    'patch_size': 18, \n",
    "\n",
    "    'fog_model_input_dim': 18*3,\n",
    "    'fog_model_dim': 320,\n",
    "    'fog_model_num_heads': 8, #6,\n",
    "    'fog_model_num_encoder_layers': 5,\n",
    "    'fog_model_num_lstm_layers': 2,\n",
    "    'fog_model_first_dropout': 0.1,\n",
    "    'fog_model_encoder_dropout': 0.1,\n",
    "    'fog_model_mha_dropout': 0.0,\n",
    "}\n",
    "\n",
    "assert CFG['block_size'] % CFG['patch_size'] == 0  # 864\n",
    "assert CFG['block_size'] % CFG['block_stride'] == 0  # 972\n",
    "\n",
    "'''\n",
    "Train and inference batch size\n",
    "\n",
    "'''\n",
    "\n",
    "GPU_BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 864, 320])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The transformer encoder layer\n",
    "'''\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=cfg['fog_model_dim'], \n",
    "                                         num_heads=cfg['fog_model_num_heads'], \n",
    "                                         dropout=cfg['fog_model_mha_dropout'])\n",
    "        \n",
    "        self.layernorm = nn.LayerNorm(cfg['fog_model_dim'])\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(cfg['fog_model_dim'], cfg['fog_model_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['fog_model_encoder_dropout']),\n",
    "            nn.Linear(cfg['fog_model_dim'], cfg['fog_model_dim']),\n",
    "            nn.Dropout(cfg['fog_model_encoder_dropout'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.mha(query=x, key=x, value=x)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm(x)\n",
    "        x = x + self.seq(x)\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "encoder_layer = EncoderLayer(CFG)\n",
    "input_tensor = torch.rand(32, 864, 320)\n",
    "output = encoder_layer(input_tensor)\n",
    "print(output.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 864, 640])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FOGEncoder is a combination of transformer encoder (D=320, H=6, L=5) and two BidirectionalLSTM layers\n",
    "\n",
    "'''\n",
    "\n",
    "class FOGEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(FOGEncoder, self).__init__()\n",
    "    \n",
    "        self.first_linear = nn.Linear(cfg['fog_model_input_dim'], cfg['fog_model_dim'])\n",
    "        \n",
    "        self.first_dropout = nn.Dropout(cfg['fog_model_first_dropout'])\n",
    "        \n",
    "        self.enc_layers = nn.Sequential()\n",
    "        for _ in range(cfg['fog_model_num_encoder_layers']):\n",
    "            self.enc_layers.append(EncoderLayer(cfg))\n",
    "            \n",
    "        self.lstm_layers = nn.LSTM(cfg['fog_model_dim'], cfg['fog_model_dim'], \n",
    "                                   num_layers=cfg['fog_model_num_lstm_layers'],\n",
    "                                   batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.sequence_len = cfg['block_size'] // cfg['patch_size'] # 864\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, self.sequence_len,\n",
    "                                                     cfg['fog_model_dim']) * 0.02, \n",
    "                                         requires_grad=True)\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        # x: (B, S, P*3), Example shape (4, 864, 54)\n",
    "        batch_size = x.size(0)\n",
    "        x = x / 25.0  # Normalization attempt in the segment [-1, 1]\n",
    "        x = self.first_linear(x)  # (batch_size, sequence_len, fog_model_dim)\n",
    "\n",
    "        if training:  # augmentation by randomly roll of the position encoding tensor\n",
    "            shifts = torch.randint(low=-self.sequence_len, high=0, size=(batch_size,))\n",
    "            random_pos_encoding = torch.cat([torch.roll(self.pos_encoding, \n",
    "                                                        shifts=s.item(), \n",
    "                                                        dims=1) \\\n",
    "                                             for s in shifts], dim=0)\n",
    "            x = x + random_pos_encoding\n",
    "        else:  # without augmentation\n",
    "            x = x + self.pos_encoding.repeat(batch_size, 1, 1)\n",
    "        \n",
    "        x = self.first_dropout(x)\n",
    "        \n",
    "        x = self.enc_layers(x) # (B,S,D)\n",
    "        \n",
    "        x, _ = self.lstm_layers(x) # (B,S,2*D)\n",
    "\n",
    "        return x\n",
    "    \n",
    "encoder = FOGEncoder(CFG)\n",
    "encoder.to(device7)\n",
    "input_tensor = torch.rand(32, 864, 54)  # (batch_size, sequence_length, input_dim)\n",
    "input_tensor = input_tensor.to(device7)\n",
    "output = encoder(input_tensor)\n",
    "print(output.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 864, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5076],\n",
       "         [0.5096]],\n",
       "\n",
       "        [[0.5075],\n",
       "         [0.5120]],\n",
       "\n",
       "        [[0.5070],\n",
       "         [0.5099]],\n",
       "\n",
       "        [[0.5105],\n",
       "         [0.5116]]], device='cuda:7', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FOGModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(FOGModel, self).__init__()\n",
    "        \n",
    "        self.encoder = FOGEncoder(cfg)\n",
    "        self.last_linear = nn.Linear(cfg['fog_model_dim'] * 2, 1)\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        # x: (B,S,input_D), e.g. (4, 864, 54)\n",
    "        x = self.encoder(x)  # (B,S,D*2), e.g. (4, 864, 640)\n",
    "        x = self.last_linear(x)  # (B,S,3), e.g. (4, 864, 3)\n",
    "        x = torch.sigmoid(x)  # Sigmoid activation\n",
    "        return x\n",
    "    \n",
    "model = FOGModel(CFG)\n",
    "model.to(device7)\n",
    "input_tensor = torch.rand(4, 864, 54)  # (batch_size, sequence_len, input_dim)\n",
    "input_tensor = input_tensor.to(device7)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)\n",
    "output[:,:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Mean-std normalization function. \n",
    "Example input: shape (5000)\n",
    "Example output: shape (5000)\n",
    "\n",
    "Used to normalize AccV, AccML, AccAP values.\n",
    "'''\n",
    "\n",
    "def sample_normalize(sample):\n",
    "    # sample: (N,)\n",
    "    if not isinstance(sample, torch.Tensor):\n",
    "        sample = torch.tensor(sample)\n",
    "    mean = torch.mean(sample)\n",
    "    std = torch.std(sample)\n",
    "    # Normalize the sample and handle division by zero\n",
    "    eps = 1e-8\n",
    "    normalized_sample = (sample - mean) / (std + eps)\n",
    "    return normalized_sample\n",
    "\n",
    "'''\n",
    "Function for splitting a series into blocks. Blocks can overlap. \n",
    "How the function works:\n",
    "Suppose we have a series with AccV, AccML, AccAP columns and len of 50000, that is (50000, 3). \n",
    "First, the series is padded so that the final length is divisible by CFG['block_size'] = 15552. Now the series shape is (62208, 3).\n",
    "Then we get blocks: first block is series[0:15552, :], second block is series[972:16524, :], ... , last block is series[46656:62208, :].\n",
    "\n",
    "'''\n",
    "\n",
    "def get_blocks(series, columns):\n",
    "    series = series.copy()\n",
    "    series = series[columns]\n",
    "    \n",
    "    block_count = math.ceil(series.shape[0] / CFG['block_size'])\n",
    "    \n",
    "    padding = block_count * CFG['block_size'] - series.shape[0]\n",
    "    padding = torch.zeros(size=(padding, series.shape[1]))\n",
    "\n",
    "    series = torch.cat([series, padding], dim=0)\n",
    "    \n",
    "    block_begins = list(range(0, series.shape[0], CFG['block_stride']))\n",
    "    block_begins = [x for x in block_begins if x + CFG['block_size'] <= series.shape[0]]\n",
    "    \n",
    "    blocks = []\n",
    "    for begin in block_begins:\n",
    "        values = series[begin:begin + CFG['block_size']]\n",
    "        blocks.append({'begin': begin,\n",
    "                       'end': begin + CFG['block_size'],\n",
    "                       'values': values})\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def folder(path): \n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "        \n",
    "def plot(e, size=(20, 4)):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.plot(e)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create train blocks with AccV, AccML, AccAP, StartHesitation, \n",
    "Turn, Walking, Valid, Mask columns and save in the directory\n",
    "\n",
    "'''\n",
    "\n",
    "save_path = '/kaggle/working/train/tdcsfog'; folder(save_path); \n",
    "\n",
    "\n",
    "# tdcsfog_metadata = pd.read_csv('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/tdcsfog_metadata.csv').set_index('Id')\n",
    "\n",
    "# blocks_descriptions = []\n",
    "# for Id in tqdm(tdcsfog_metadata.index, total=len(tdcsfog_metadata.index), desc='Preparing'):\n",
    "#     series = pd.read_csv(f'/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{Id}.csv')\n",
    "    \n",
    "#     series['AccV'] = sample_normalize(series['AccV'].values)\n",
    "#     series['AccML'] = sample_normalize(series['AccML'].values)\n",
    "#     series['AccAP'] = sample_normalize(series['AccAP'].values)\n",
    "#     series['Valid'] = 1\n",
    "#     series['Mask'] = 1\n",
    "\n",
    "#     blocks = get_blocks(series, ['AccV', 'AccML', 'AccAP', 'StartHesitation', 'Turn', 'Walking', 'Valid', 'Mask'])\n",
    "\n",
    "#     for block_count, block in enumerate(blocks):\n",
    "#         fname, values = f'{Id}_{block_count}.npy', block['values']\n",
    "#         block_description = {}\n",
    "#         block_description['Id'] = Id\n",
    "#         block_description['Count'] = block_count\n",
    "#         block_description['File'] = fname\n",
    "#         block_description['Path'] = f'{save_path}/{fname}'\n",
    "#         block_description['Source'] = 'tsfog'\n",
    "#         block_description['StartHesitation_size'] = np.sum(values[:, 3])\n",
    "#         block_description['Turn_size'] = np.sum(values[:, 4])\n",
    "#         block_description['Walking_size'] = np.sum(values[:, 5])\n",
    "#         block_description['Valid_size'] = np.sum(values[:, 6])\n",
    "#         block_description['Mask_size'] = np.sum(values[:, 7])\n",
    "\n",
    "#         blocks_descriptions.append(block_description)\n",
    "#         np.save(f'{save_path}/{fname}', values)\n",
    "\n",
    "# blocks_descriptions = pd.DataFrame(blocks_descriptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
